{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FChlxcjT4ldq","executionInfo":{"status":"ok","timestamp":1731569442140,"user_tz":-540,"elapsed":32449,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}},"outputId":"8af1dbcb-bb6a-4ae9-ff9b-dff06d887b71"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-11-14 07:30:07--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  5.25MB/s    in 21s     \n","\n","2024-11-14 07:30:29 (3.76 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"]}],"source":["!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","import os\n","\n","def load_data(path):\n","    texts = []\n","    labels = []\n","    for label_type in ['neg', 'pos']:\n","        dir_name = os.path.join(path, label_type)\n","        for fname in os.listdir(dir_name):\n","            if fname.endswith('.txt'):\n","                with open(os.path.join(dir_name, fname), 'r', encoding='utf-8') as f:\n","                    texts.append(f.read())\n","                labels.append(0 if label_type == 'neg' else 1)\n","    return texts, labels\n","\n","train_texts, train_labels = load_data('aclImdb/train')\n","test_texts, test_labels = load_data('aclImdb/test')"]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKnCM8guljnx","executionInfo":{"status":"ok","timestamp":1731569545551,"user_tz":-540,"elapsed":5012,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}},"outputId":"7109bd43-3da8-43be-ac8c-99bcbf20b2ce"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aypJDiUGlq3n","executionInfo":{"status":"ok","timestamp":1731569569178,"user_tz":-540,"elapsed":1045,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}},"outputId":"c739ee92-882d-4fbd-a044-44b990553e7e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import re\n","#from nltk.tokenize import word_tokenize\n","#import nltk\n","#nltk.download('punkt')\n","\n","def preprocess(text):\n","    text = re.sub(r\"<.*>\", \"\", text)\n","    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n","    text = text.lower()\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","train_tokens = [preprocess(text) for text in train_texts]\n","test_tokens = [preprocess(text) for text in test_texts]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":730},"id":"ha4scwdN5Ohj","executionInfo":{"status":"error","timestamp":1731569592557,"user_tz":-540,"elapsed":496,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}},"outputId":"5651c0b9-9444-4ce1-b48a-83ef6fdba655"},"execution_count":5,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2a60a8f30ac9>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2a60a8f30ac9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2a60a8f30ac9>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","all_tokens = [token for tokens in train_tokens for token in tokens]\n","word_counts = Counter(all_tokens)\n","vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 5]\n","word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n","idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n","\n","vocab_size = len(vocab)\n","print(vocab_size)"],"metadata":{"id":"zJjl6pLN5sba","executionInfo":{"status":"aborted","timestamp":1731569447334,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_seq_len = 200\n","\n","def tokens_to_indices(tokens_list, word_to_idx, max_seq_len):\n","    sequences = []\n","    for tokens in tokens_list:\n","        seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n","        if len(seq) < max_seq_len:\n","            seq += [word_to_idx['<PAD>']] * (max_seq_len - len(seq))\n","        else:\n","            seq = seq[:max_seq_len]\n","        sequences.append(seq)\n","    return sequences\n","\n","train_sequences = tokens_to_indices(train_tokens, word_to_idx, max_seq_len)\n","test_sequences = tokens_to_indices(test_tokens, word_to_idx, max_seq_len)"],"metadata":{"id":"Fl3g3Swg6adX","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":8,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TextDataset(Dataset):\n","    def __init__(self, sequences, labels):\n","        super().__init__()\n","        self.sequences = sequences\n","        self.labels = labels\n","    def __len__(self):\n","        return len(self.sequences)\n","    def __getitem__(self, idx):\n","        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n","        label = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return sequence, label"],"metadata":{"id":"--3JgN_76yNe","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":8,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TextDataset(train_sequences, train_labels)\n","test_dataset = TextDataset(test_sequences, test_labels)\n","\n","batch_size = 64\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"WS8WTZYt7LbH","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":8,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","class SentimentLSTM(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, (hidden, cell) = self.lstm(embedded)\n","        out = self.fc(hidden[-1])\n","        out = self.sigmoid(out)\n","        return out.squeeze()"],"metadata":{"id":"0Zbz2_IA7Y_N","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_size=128\n","hidden_size=128\n","output_size=1\n","num_layers=2\n","num_epochs=5\n","learning_rate = 0.001\n","\n","model = SentimentLSTM(vocab_size, embed_size, hidden_size, output_size, num_layers)\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"jg_Yaox178UA","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for sequences, labels in train_loader:\n","        sequences, labels = sequences.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')"],"metadata":{"id":"0QcADzLy8Or6","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for sequences, labels in test_loader:\n","        sequences, labels = sequences.to(device), labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs > 0.5).float()\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    print(f'테스트 정확도: {100 * correct / total:.2f}%')"],"metadata":{"id":"1NnGWdoh8hep","executionInfo":{"status":"aborted","timestamp":1731569447335,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_sentiment(text):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    tokens = preprocess(text)\n","    seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n","    if len(seq) < max_seq_len:\n","        seq += [word_to_idx['<PAD>']]*(max_seq_len - len(seq))\n","    else:\n","        seq = seq[:max_seq_len]\n","    sequence = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(sequence)\n","        predicted = '긍정' if output.item() >= 0.5 else '부정'\n","        print(f'입력 문장: {text}')\n","        print(f'예측 확률: {output.item():.4f}')\n","        print(f'예측 결과: {predicted}')"],"metadata":{"id":"a-1R0aqF-U6q","executionInfo":{"status":"aborted","timestamp":1731569447336,"user_tz":-540,"elapsed":7,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sentence = \"This movie was fantastic! I really enjoyed it.\"\n","predict_sentiment(test_sentence)"],"metadata":{"id":"6ItKUy05_GZs","executionInfo":{"status":"aborted","timestamp":1731569447337,"user_tz":-540,"elapsed":8,"user":{"displayName":"KATE KIM","userId":"08191676103652969482"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uAsBhmuX_KkQ"},"execution_count":null,"outputs":[]}]}

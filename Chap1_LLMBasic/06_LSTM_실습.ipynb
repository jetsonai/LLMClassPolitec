{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch LSTM을 이용한 감정 분석 모델 구현"
      ],
      "metadata": {
        "id": "lV_lamiJIMCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 준비 및 전처리\n",
        "  - 데이터 다운로드 및 로드\n",
        "  - 전처리\n",
        "  - 데이터셋 분할 및 DataLoader 생성\n",
        "- 모델 구축\n",
        "  - LSTM 기반 감정 분석 모델 정의\n",
        "- 모델 학습\n",
        "  - 손실 함수 및 최적화기 설정\n",
        "  - 모델 학습 루프 구현\n",
        "- 모델 평가\n",
        "  - 테스트 데이터로 평가\n",
        "- 예측 및 결과 확인\n",
        "  - 임의 문장으로 예측 수행"
      ],
      "metadata": {
        "id": "sp7YmpIgIPNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비 및 전처리"
      ],
      "metadata": {
        "id": "l43IMqFkIXLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 다운로드 및 로드"
      ],
      "metadata": {
        "id": "mUk3JS-FIe6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qET7fLVCIFwj"
      },
      "outputs": [],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "import os\n",
        "\n",
        "def load_data(path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_type in ['neg', 'pos']:\n",
        "        dir_name = os.path.join(path, label_type)\n",
        "        for fname in os.listdir(dir_name):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(dir_name, fname), 'r', encoding='utf-8') as f:\n",
        "                    texts.append(f.read())\n",
        "                labels.append(0 if label_type == 'neg' else 1)\n",
        "    return texts, labels\n",
        "\n",
        "# 훈련 데이터 로드\n",
        "train_texts, train_labels = load_data('aclImdb/train')\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_texts, test_labels = load_data('aclImdb/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리"
      ],
      "metadata": {
        "id": "pPOY7ZQ-Ihvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess(text):\n",
        "    # HTML 태그 제거\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    # 알파벳 이외의 문자 제거\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    # 소문자 변환\n",
        "    text = text.lower()\n",
        "    # 토큰화\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# 훈련 데이터 전처리\n",
        "train_tokens = [preprocess(text) for text in train_texts]\n",
        "\n",
        "# 테스트 데이터 전처리\n",
        "test_tokens = [preprocess(text) for text in test_texts]"
      ],
      "metadata": {
        "id": "Plss0PBYIbpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 모든 단어를 모아서 빈도수 계산\n",
        "all_tokens = [token for tokens in train_tokens for token in tokens]\n",
        "word_counts = Counter(all_tokens)\n",
        "\n",
        "# 가장 많이 등장한 단어 순으로 정렬하여 단어 사전 생성\n",
        "vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= 5]\n",
        "\n",
        "# 단어와 인덱스를 매핑\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"단어 사전 크기: {vocab_size}\")"
      ],
      "metadata": {
        "id": "TERnFH9iI1c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 시퀀스 길이 설정\n",
        "max_seq_len = 200\n",
        "\n",
        "def tokens_to_indices(tokens_list, word_to_idx, max_seq_len):\n",
        "    sequences = []\n",
        "    for tokens in tokens_list:\n",
        "        seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "        if len(seq) < max_seq_len:\n",
        "            seq += [word_to_idx['<PAD>']] * (max_seq_len - len(seq))\n",
        "        else:\n",
        "            seq = seq[:max_seq_len]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "# 훈련 데이터 변환\n",
        "train_sequences = tokens_to_indices(train_tokens, word_to_idx, max_seq_len)\n",
        "\n",
        "# 테스트 데이터 변환\n",
        "test_sequences = tokens_to_indices(test_tokens, word_to_idx, max_seq_len)"
      ],
      "metadata": {
        "id": "rmMgBA1tI-lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 분할 및 DataLoader 생성"
      ],
      "metadata": {
        "id": "IRVnS8EHIqO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return sequence, label"
      ],
      "metadata": {
        "id": "CYzbo5LLItDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 생성\n",
        "train_dataset = TextDataset(train_sequences, train_labels)\n",
        "test_dataset = TextDataset(test_sequences, test_labels)\n",
        "\n",
        "# DataLoader 생성\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "4VT0EvJ4Ivx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구축"
      ],
      "metadata": {
        "id": "JdM1Gok9Ivko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len]\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded: [batch_size, seq_len, embed_size]\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        # lstm_out: [batch_size, seq_len, hidden_size]\n",
        "        # hidden: [num_layers, batch_size, hidden_size]\n",
        "        out = self.fc(hidden[-1])\n",
        "        # out: [batch_size, output_size]\n",
        "        out = self.sigmoid(out)\n",
        "        return out.squeeze()"
      ],
      "metadata": {
        "id": "5mNsAnCwJD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "wM9kZJ77JHrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실 함수 및 최적화기 설정"
      ],
      "metadata": {
        "id": "hH7OZ_6BJUIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "embed_size = 128\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# 모델 초기화\n",
        "model = SentimentLSTM(vocab_size, embed_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# 손실 함수와 최적화기 정의\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "D1dGSNG-JG3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습 루프 구현"
      ],
      "metadata": {
        "id": "m_qj94fRJV0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for sequences, labels in train_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 모델 예측\n",
        "        outputs = model(sequences)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "zcLG9kD4JOIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가"
      ],
      "metadata": {
        "id": "LGUJFZOyJaDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        outputs = model(sequences)\n",
        "        predicted = (outputs >= 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f\"테스트 정확도: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "cUU8toIdJemU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측 및 결과 확인"
      ],
      "metadata": {
        "id": "0vJ2bBEGJbKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokens = preprocess(text)\n",
        "    seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "    if len(seq) < max_seq_len:\n",
        "        seq += [word_to_idx['<PAD>']] * (max_seq_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_seq_len]\n",
        "    sequence = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(sequence)\n",
        "        prediction = '긍정' if output.item() >= 0.5 else '부정'\n",
        "        print(f\"입력 문장: {text}\")\n",
        "        print(f\"예측 확률: {output.item():.4f}\")\n",
        "        print(f\"예측 결과: {prediction}\")"
      ],
      "metadata": {
        "id": "io-u0G5kJW3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임의 문장으로 예측 수행"
      ],
      "metadata": {
        "id": "o-BlkQ35Jk62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 문장\n",
        "test_sentence = \"This movie was fantastic! I really enjoyed it.\"\n",
        "predict_sentiment(test_sentence)"
      ],
      "metadata": {
        "id": "oELS7q1FJjhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"lstm_5epoch.pt\")"
      ],
      "metadata": {
        "id": "Sr7m5lA1JlYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = SentimentLSTM(vocab_size, embed_size, hidden_size, output_size, num_layers)\n",
        "model2.load_state_dict(torch.load(\"lstm_5epoch.pt\"))\n",
        "model2.eval()\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokens = preprocess(text)\n",
        "    seq = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "    if len(seq) < max_seq_len:\n",
        "        seq += [word_to_idx['<PAD>']] * (max_seq_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_seq_len]\n",
        "    sequence = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    model2.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(sequence)\n",
        "        prediction = '긍정' if output.item() >= 0.5 else '부정'\n",
        "        print(f\"입력 문장: {text}\")\n",
        "        print(f\"예측 확률: {output.item():.4f}\")\n",
        "        print(f\"예측 결과: {prediction}\")\n",
        "\n",
        "test_sentence = \"This movie was fantastic! I really enjoyed it.\"\n",
        "predict_sentiment(test_sentence)"
      ],
      "metadata": {
        "id": "HbqMRgM9Werv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CUDCSrKWqPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
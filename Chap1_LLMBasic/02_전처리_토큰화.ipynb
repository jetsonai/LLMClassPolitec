{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화 (Tokenization)"
      ],
      "metadata": {
        "id": "P55VJAjTe4GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트를 분석하기 위해 최소한의 의미를 가지는 가장 작은 단위인 토큰으로 분리하는 과정\n",
        "- 언어별로 토큰화 방법이 다르며, 한국어의 경우 형태소 분석을 병행하는 경우가 많음\n",
        "- 중요성\n",
        " - 텍스트 분할\n",
        "   - 긴 글을 단어, 문장 혹은 의미 단위로 분리하여 모델이 각 단어의 의미를 파악할 수 있게 함\n",
        " - 후속 처리의 기초\n",
        "   - 불용어 제거, 정규화 등의 이후 단계에서 각 토큰을 개별적으로 다루기 때문에 필수적임\n"
      ],
      "metadata": {
        "id": "NO8Nk0U3fH6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 예시\n",
        "  - Machine learning methods including ANN have been applied in compound activity prediction for a long time.\n",
        "    - Machine / learning / methods / including / ANN / have / been / applied / in / compound / activity / prediction / for / a / long / time.\n",
        "\n",
        "  - 어제 한화가 롯데에게 9:15로 지고 말았다.\n",
        "    - 어제 / 한화 / 가 / 롯데 / 에게 / 9:15 / 로  / 지고 / 말았다.\n",
        "    - 구두점이나 특수문자를 전부 제거하는 작업만으로는 토큰화를 진행하기 어려울 수 있음\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RLIDswfKfQki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 토큰화 (Word Tokenization)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rRd_0Dy_fYUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트를 단어 단위로 분리함\n",
        "- 영문 단어 토큰화\n",
        "  - 공백과 구두점 기반 분리\n",
        "    - 영어는 단어 간에 공백이 있고, 구두점을 통해 문장의 경계를 나타냄\n",
        "  - 접어 및 축약어 처리\n",
        "    - 영어에서는 don’t, I’m과 같은 축약어가 있어 이를 적절히 처리해야 함\n",
        "  - 특수문자 및 숫자 처리\n",
        "    - 이메일 주소, URL, 숫자 등의 처리방안"
      ],
      "metadata": {
        "id": "l13AcaYCfzca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 공백 기반 토큰화"
      ],
      "metadata": {
        "id": "Vvs7dyh6gQ_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world! This is a test. Let's see how it tokenizes words.\"\n",
        "tokens = text.split()\n",
        "print(\"토큰화 결과:\", tokens)"
      ],
      "metadata": {
        "id": "JKedY-A8gPgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단순히 공백을 기준으로 분리하기 때문에 구두점과 축약어가 제대로 처리되지 않음\n",
        "- 일반적인 NLP 작업에는 적합하지 않음"
      ],
      "metadata": {
        "id": "EsPmedAugXVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nltk 라이브러리를 활용한 토큰화"
      ],
      "metadata": {
        "id": "alXs802zgqI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # 토큰화를 위한 데이터 다운로드, Colab, 우분투"
      ],
      "metadata": {
        "id": "wTIQnHTqhT9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Hello, world! This is a test. Let's see how it tokenizes words.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"토큰화 결과:\", tokens)"
      ],
      "metadata": {
        "id": "I-k5hDlVg0ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk 설명\n",
        "- 구두점 처리\n",
        "  - NLTK의 word_tokenize 함수는 구두점을 별도의 토큰으로 인식함\n",
        "- 축약어 처리\n",
        "  - Let’s가 Let과 ‘s로 분리됨\n",
        "- 숫자 및 특수문자\n",
        "  - 숫자나 특수문자도 토큰으로 분리됨"
      ],
      "metadata": {
        "id": "AmpmvJ70gyUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spacy 라이브러리를 활용한 토큰화"
      ],
      "metadata": {
        "id": "_d_KL7OThEiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm  # 영어 모델 다운로드"
      ],
      "metadata": {
        "id": "RHcWhx7whR1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Hello, world! This is a test. Let's see how it tokenizes words.\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"토큰화 결과:\", tokens)"
      ],
      "metadata": {
        "id": "7XjY-PQahHW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy 설명\n",
        "- NLTK와 결과가 유사함\n",
        "- 추가로 품사 태깅이나 개체명 인식 등의 기능을 함께 사용할 수 있음\n"
      ],
      "metadata": {
        "id": "c35ka4hJhNTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 형태소 토큰화 (Morphological Tokenization)"
      ],
      "metadata": {
        "id": "oFR6qZ9xf5EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어를 구성하는 최소 의미 단위인 형태소로 분리\n",
        "- 한국어 형태소 토큰화\n",
        "  - 교착어 특성\n",
        "    - 조사와 어미가 단어에 붙어 있어 형태소 분석이 필요\n",
        "  - 띄어쓰기의 불규칙성\n",
        "    - 띄어쓰기가 영어처럼 엄격하지 않으며, 원시 데이터에 띄어쓰기 오류가 빈번히 발생\n",
        "  - 복합어와 신조어 처리\n",
        "    - 새로운 단어와 복합어를 정확히 분리하고 인식하기 어려움\n"
      ],
      "metadata": {
        "id": "1HPlAMsef7ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국어의 토큰화 과정에서 형태소는 중요한 역할을 담당\n",
        "- 화분에 예쁜 꽃이 피었다.\n",
        "    - 화분(명사) + 에(조사) + 예쁘(어간) + ㄴ(어미) + 꽃(명사) + 이(조사) + 피(어간) + 었(어미) + 다(어미)\n",
        "    - 자립 형태소: 명사, 수사, 부사, 감탄사\n",
        "    - 의존 형태소: 조사, 어미 어간\n"
      ],
      "metadata": {
        "id": "8EB61I6LgCZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한국어 형태소 분석 라이브러리\n",
        "\n"
      ],
      "metadata": {
        "id": "1niIeYRRh6hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KoNLPy\n",
        "- Okt (Open Korea Text)\n",
        "- Kkma\n",
        "- Komoran\n",
        "- Hannanum"
      ],
      "metadata": {
        "id": "CEC98ziKiBw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 형태소 분석기의 사전과 알고리즘이 다르기 때문에, 형태소 분석기마다 결과가 다를 수 있음\n",
        "\n",
        "형태소 추출: 텍스트를 형태소 단위로 분리\n",
        "\n",
        "품사 태깅: 각 형태소에 품사 태그를 부착\n",
        "\n",
        "명사 추출: 텍스트에서 명사를 추출\n"
      ],
      "metadata": {
        "id": "sfXwksO4iKaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "6ysW2smLiFF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Okt를 이용한 형태소 토큰화"
      ],
      "metadata": {
        "id": "D-MkbP2ziVAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"한국어 자연어 처리는 어렵지만 재미있습니다!\"\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = okt.morphs(text)\n",
        "print(\"형태소:\", morphs)\n",
        "\n",
        "# 품사 태깅\n",
        "pos = okt.pos(text)\n",
        "print(\"품사 태깅:\", pos)\n",
        "\n",
        "# 명사 추출\n",
        "nouns = okt.nouns(text)\n",
        "print(\"명사:\", nouns)"
      ],
      "metadata": {
        "id": "1LVzCXvkiYGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kkma를 이용한 형태소 토큰화"
      ],
      "metadata": {
        "id": "0tjYDNqZidFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"한국어 자연어 처리는 어렵지만 재미있습니다!\"\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = kkma.morphs(text)\n",
        "print(\"형태소:\", morphs)\n",
        "\n",
        "# 품사 태깅\n",
        "pos = kkma.pos(text)\n",
        "print(\"품사 태깅:\", pos)\n",
        "\n",
        "# 명사 추출\n",
        "nouns = kkma.nouns(text)\n",
        "print(\"명사:\", nouns)"
      ],
      "metadata": {
        "id": "Em5bMM4Vihfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Komoran을 이용한 형태소 토큰화"
      ],
      "metadata": {
        "id": "Gtc0Fa4YifkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"한국어 자연어 처리는 어렵지만 재미있습니다!\"\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = komoran.morphs(text)\n",
        "print(\"형태소:\", morphs)\n",
        "\n",
        "# 품사 태깅\n",
        "pos = komoran.pos(text)\n",
        "print(\"품사 태깅:\", pos)\n",
        "\n",
        "# 명사 추출\n",
        "nouns = komoran.nouns(text)\n",
        "print(\"명사:\", nouns)"
      ],
      "metadata": {
        "id": "LJfSBVwvinGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hannanum을 이용한 형태소 토큰화"
      ],
      "metadata": {
        "id": "hljZulOdipnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"한국어 자연어 처리는 어렵지만 재미있습니다!\"\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = hannanum.morphs(text)\n",
        "print(\"형태소:\", morphs)\n",
        "\n",
        "# 품사 태깅\n",
        "pos = hannanum.pos(text)\n",
        "print(\"품사 태깅:\", pos)\n",
        "\n",
        "# 명사 추출\n",
        "nouns = hannanum.nouns(text)\n",
        "print(\"명사:\", nouns)"
      ],
      "metadata": {
        "id": "NYoLmN-DisaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문장 토큰화 (Sentence Tokenization)"
      ],
      "metadata": {
        "id": "Vx9c3TOSgI04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트를 문장 단위로 분리\n",
        "- 일반적으로 마침표, 느낌표, 물음표 등을 기준으로 분리\n",
        "- 한국어에서는 따옴표나 줄임표 등의 처리에 주의해야 함\n",
        "- 문장 토큰화 역시 다양한 방법으로 수행할 수 있음"
      ],
      "metadata": {
        "id": "4aljXY9HgJn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regex를 이용한 문장 토큰화"
      ],
      "metadata": {
        "id": "pUbZCPzMivo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world! How are you doing today? This is an example of sentence tokenization. Let's see how it works. BTW, Dr. Kim has not medical doctor.\"\n",
        "\n",
        "# 정규식을 이용한 문장 토큰화\n",
        "sentence_endings = re.compile(r'[.!?]')\n",
        "sentences = sentence_endings.split(text)\n",
        "\n",
        "# 공백 제거\n",
        "sentences = [sentence.strip() for sentence in sentences if sentence.strip() != '']\n",
        "\n",
        "print(\"문장 토큰화 결과:\")\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    print(f\"문장 {idx+1}: {sentence}\")"
      ],
      "metadata": {
        "id": "sXMtqL27fHbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정규식을 사용하여 마침표, 느낌표, 물음표를 기준으로 텍스트를 분리\n",
        "- 간단하지만 약어 처리 등에서 한계가 있음"
      ],
      "metadata": {
        "id": "W-KPZrXKi3mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### nltk를 이용한 문장 토큰화"
      ],
      "metadata": {
        "id": "ND3VtvPDi-hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # 토큰화를 위한 데이터 다운로드"
      ],
      "metadata": {
        "id": "ODDeE11Ri1nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello, world! How are you doing today? This is an example of sentence tokenization. Let's see how it works. BTW, Dr. Kim has not medical doctor.\"\n",
        "\n",
        "# 문장 토큰화 수행\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"문장 토큰화 결과:\")\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    print(f\"문장 {idx+1}: {sentence}\")"
      ],
      "metadata": {
        "id": "_7wE6gwKjjAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sent_tokenize 함수는 주어진 텍스트를 문장 단위로 분리\n",
        "- 내부적으로 PunktSentenceTokenizer를 사용하여 문장 경계를 인식\n",
        "- 약어와 마침표 등을 고려하여 정확한 문장 분리를 수행"
      ],
      "metadata": {
        "id": "DN4bTuwPjsvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### spacy를 이용한 문장 토큰화"
      ],
      "metadata": {
        "id": "Uk3AI8m5jwoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm  # 영어 모델 다운로드"
      ],
      "metadata": {
        "id": "jcqs0t4ij2vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Hello, world! How are you doing today? This is an example of sentence tokenization. Let's see how it works. BTW, Dr. Kim has not medical doctor.\"\n",
        "\n",
        "# 문장 토큰화 수행\n",
        "doc = nlp(text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "print(\"문장 토큰화 결과:\")\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    print(f\"문장 {idx+1}: {sentence.text}\")"
      ],
      "metadata": {
        "id": "9CTCI4yxjyyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- spacy의 nlp 객체를 통해 텍스트를 처리하고, doc.sents를 사용하여 문장 단위로 분리\n",
        "- 언어 모델에 기반하여 문장 경계를 인식하므로 정확도가 높음"
      ],
      "metadata": {
        "id": "u_B_RMcsjzBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한국어 문장 토큰화 예제"
      ],
      "metadata": {
        "id": "F0xmJGuWj8G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "id": "Vrshc4DqkBQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "text = \"안녕하세요. 오늘은 자연어 처리를 배웁니다! 기쁘지 않나요? 저는 정말 기대돼요.\"\n",
        "\n",
        "# 문장 토큰화 수행\n",
        "sentences = kss.split_sentences(text)\n",
        "\n",
        "print(\"문장 토큰화 결과:\")\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    print(f\"문장 {idx+1}: {sentence}\")"
      ],
      "metadata": {
        "id": "xoZPD5JVjmZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- kss (Korean sentence splitter)는 한국어 문장 분리를 위한 라이브러리\n",
        "- 한국어의 특성을 고려하여 문장 토큰화 수행\n"
      ],
      "metadata": {
        "id": "id2nKR46kGb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습"
      ],
      "metadata": {
        "id": "T-SQO2fLkZIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 토큰화 적용하기"
      ],
      "metadata": {
        "id": "0ICJuuIZkmBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 실습한 내용을 참고하여 주어진 문장을 단어 단위로 토큰화 하세요.\n",
        "\n",
        "NLTK의 work_tokenize 함수를 사용하지 않고 정규식을 이용하여 토큰화를 구현해보세요.\n",
        "\n",
        "\n",
        "예시 입력\n",
        "\n",
        "```\n",
        "text = \"Hello! How are you doing today? I'm doing fine.\"\n",
        "```\n",
        "\n",
        "목표 출력\n",
        "\n",
        "```\n",
        "['Hello', 'How', 'are', 'you', 'doing', 'today', 'I', \"'m\", 'doing', 'fine']\n",
        "```\n"
      ],
      "metadata": {
        "id": "F2U-n6UJke1m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tgJ5Back6N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 토큰화 적용하기"
      ],
      "metadata": {
        "id": "eEFP9u_6ko3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 실습한 내용을 참고하여 주어진 텍스트를 문장 단위로 토큰화하세요.\n",
        "\n",
        "NLTK의 sent_tokenize 함수를 사용하세요\n",
        "\n",
        "예시 입력\n",
        "```\n",
        "text = \"Mr. Smith went to Washington. He arrived on Jan. 15th. It was a sunny day.\"\n",
        "```\n",
        "\n",
        "목표 출력\n",
        "```\n",
        "['Mr. Smith went to Washington.', 'He arrived on Jan. 15th.', 'It was a sunny day.']\n",
        "```"
      ],
      "metadata": {
        "id": "t5ymEVnQk5pN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HppC3tiBkHzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 토큰화 적용하기"
      ],
      "metadata": {
        "id": "fbdc4K8xlP8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 실습한 내용을 참고하여 주어진 한국어 문장을 형태소 단위로 토큰화하고, 명사만 추출하세요.\n",
        "\n",
        "KoNLPy의 Okt 형태소 분석기를 사용하세요\n",
        "\n",
        "예시 입력\n",
        "```\n",
        "text = \"자연어 처리는 인공지능의 한 분야입니다.\"\n",
        "```\n",
        "\n",
        "목표 출력\n",
        "```\n",
        "['자연어', '처리', '인공지능', '분야']\n",
        "```"
      ],
      "metadata": {
        "id": "7PQ7j6nelOsO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMWq6M0slPlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 토큰화 적용하기"
      ],
      "metadata": {
        "id": "H08rWzvwlRxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 실습한 내용을 참고하여 정규식을 사용하여 이메일 주소와 URL을 특수 토큰 <EMAIL>과 <URL>로 대체하고, 공백을 기준으로 토큰화하세요.\n",
        "\n",
        "예시 입력\n",
        "```\n",
        "text = \"문의사항이 있으시면 example@example.com 또는 http://example.com 으로 연락주세요.\"\n",
        "```\n",
        "목표 출력\n",
        "```\n",
        "['문의사항이', '있으시면', '<EMAIL>', '또는', '<URL>', '으로', '연락주세요']\n",
        "```"
      ],
      "metadata": {
        "id": "Wkn5F_OvlfiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXLLtfvBlew4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
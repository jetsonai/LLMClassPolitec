{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 벡터화 (Vectorization)"
      ],
      "metadata": {
        "id": "k4s801njC79i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 인코딩이라고도 함\n",
        "\n",
        "대표적인 벡터화 방법\n",
        "\n",
        "- Bag-of-Words (BoW)\n",
        "  - 단순 빈도 기반, 텍스트 내 단어 등장 횟수를 벡터로 표현\n",
        "- TF-IDF\n",
        "  - 단어의 빈도와 역문서 빈도를 반영하여 가중치 부여\n",
        "- 단어 임베딩 (Word Embeddings)\n",
        "  - Word2Vec, GloVe\n",
        "  - 단어의 의미적 관계를 고려한 밀집 벡터\n",
        "- 문맥 기반 임베딩 (Contextual Embeddings)\n",
        "  - BERT 등\n",
        "  - 문맥에 따라 동적으로 변하는 단어 벡터를 생성\n"
      ],
      "metadata": {
        "id": "aEjKC6RVDDMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words (BoW)"
      ],
      "metadata": {
        "id": "bjqJMf_tDI3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "핵심 아이디어\n",
        "- 문서 내 단어의 등장 횟수를 기반으로 벡터를 생성\n",
        "\n",
        "특징\n",
        "- 단어의 순서는 고려하지 않음\n",
        "- 단순하지만, 단어의 빈도 정보를 반영할 수 있음\n",
        "- 희소한 (high-dimensional sparse) 벡터를 생성하는 경우가 많음"
      ],
      "metadata": {
        "id": "kOEi8YiQDMin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# CountVectorizer를 통해 BoW 벡터 생성\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag-of-Words Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "YEPUZPh3Ddtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF (Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "dSz34XQwDoUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "핵심 아이디어\n",
        "- 단순한 빈도수 외에도 단어가 문서 내에서 얼마나 중요한지를 반영\n",
        "\n",
        "특징\n",
        "- TF (Term Frequency)\n",
        "  - 특정 단어가 문서에 얼마나 자주 등장하는지\n",
        "- IDF (Inverse Document Frequency)\n",
        "  - 단어가 전체 문서 집합에서 얼마나 희귀한지를 계산하여 흔한 단어의 영향력을 줄임\n",
        "- 결과적으로 각 단어에 가중치를 부여하여 벡터화"
      ],
      "metadata": {
        "id": "MybtYkkoDq9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documents = [\n",
        "    \"자연어 처리는 재미있다\",\n",
        "    \"자연어 처리는 어려운 만큼 재미있다\",\n",
        "    \"나는 자연어 처리를 공부한다\",\n",
        "    \"데이터 분석도 공부한다\"\n",
        "]\n",
        "# TF-IDF 벡터라이저 객체 생성\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 문서들을 벡터화\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# 단어 목록 출력\n",
        "print(\"단어 목록:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# TF-IDF 매트릭스 출력\n",
        "print(\"TF-IDF 매트릭스:\\n\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "id": "jhz_tkZiEIZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 임베딩 (Word Embeddings)"
      ],
      "metadata": {
        "id": "AEduF2q2EXCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "핵심 아이디어\n",
        "- 각 단어를 고정 길이의 밀집 벡터로 변환하며, 단어 간 의미적 유사도를 반영\n",
        "\n",
        "대표 모델\n",
        "- Word2Vec, GloVe 등\n",
        "\n",
        "특징\n",
        "- 단어 사이의 의미적 관계를 수치적으로 표현\n",
        "- 단어 벡터의 차원이 일반적으로 BoW나 TF-IDF보다 훨씬 낮으며, 연산 효율성이 좋음"
      ],
      "metadata": {
        "id": "AbSaDpvfEcVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec (https://arxiv.org/abs/1301.3781)"
      ],
      "metadata": {
        "id": "bBbdjxE5EyT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW(Continuous Bag of Words)\n",
        "- 주변 단어들을 이용하여 중심 단어를 예측\n",
        "- 주변 단어들의 문맥 정보를 활용\n",
        "\n",
        "예시\n",
        "\n",
        "```\n",
        "예문: \"나는 __를 좋아합니다.\"\n",
        "주변 단어: ['나는', '를', '좋아합니다']\n",
        "중심 단어: '__'\n",
        "```\n",
        "\n",
        "Skip-Gram\n",
        "- 중심 단어를 이용하여 주변 단어들을 예측\n",
        "- 희귀 단어에 대해 더 좋은 성능을 보임\n",
        "\n",
        "예시\n",
        "\n",
        "```\n",
        "중심 단어: '자연어'\n",
        "주변 단어: ['나는', '처리를', '공부합니다']\n",
        "```"
      ],
      "metadata": {
        "id": "Ff5_OBvrFBt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 라이브러리 준비\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "pX7CJwhXF3wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비\n",
        "sentences = [\n",
        "    \"나는 자연어 처리를 공부합니다\",\n",
        "    \"자연어 처리는 재미있습니다\",\n",
        "    \"딥러닝을 이용한 자연어 처리\",\n",
        "    \"자연어 처리와 딥러닝은 인공지능의 핵심 분야입니다\"\n",
        "]\n",
        "# Tokenization\n",
        "sentences = [x.split(\" \") for x in sentences]"
      ],
      "metadata": {
        "id": "8SAr-KzyGE7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화 및 학습\n",
        "model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "3KY5ovhfGHXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 단어 벡터 확인\n",
        "word_vectors = model.wv"
      ],
      "metadata": {
        "id": "M0aJfcyPGNrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '자연어' 단어 벡터 확인\n",
        "print(\"'자연어' 단어 벡터:\")\n",
        "print(word_vectors['자연어'])"
      ],
      "metadata": {
        "id": "gJ75wobqGQ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '자연어'와 유사한 단어 찾기\n",
        "similar_words = word_vectors.most_similar('자연어')\n",
        "print(\"'자연어'와 유사한 단어들:\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "id": "HB5ucGoAGR3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 단어 간 유사도 계산\n",
        "similarity = word_vectors.similarity('자연어', '처리')\n",
        "print(f\"'자연어'와 '처리'의 유사도: {similarity}\")"
      ],
      "metadata": {
        "id": "G3W70BhWGSzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GloVe (Global Vectors for Word Representation) (https://nlp.stanford.edu/pubs/glove.pdf)"
      ],
      "metadata": {
        "id": "O_kTwqW4FEwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "전역 통계 활용\n",
        "- 동시 발생 행렬\n",
        "  - 전체 말뭉치에서 단어들이 서로 몇 번 같이 등장하는지를 집계하여 동시 발생 행렬을 제작\n",
        "- 전역 정보 반영\n",
        "  - Word2Vec과 같이 로컬 문맥(window)만을 고려하는 방법과 달리, GloVe는 전체 코퍼스의 통계 정보를 사용하여 단어 간 관계를 학습\n",
        "\n",
        "수학적 모델\n",
        "- GloVe는 단어 벡터들의 내적이 두 단어가 함께 등장할 확률의 로그와 가까워지도록 하는 회귀 문제로 접근\n",
        "- 이를 위해 가중치 함수(weighting function)를 사용하여 자주 등장하는 단어 쌍과 드물게 등장하는 단어 쌍에 대해 균형 잡힌 학습을 수행\n",
        "\n",
        "선형 관계 포착\n",
        "- 학습된 단어 벡터들은 단순한 의미적 유사도뿐만 아니라, 벡터 연산(예: \"king\" - \"man\" + \"woman\" ≈ \"queen\")과 같은 선형 관계를 잘 반영하는 특징이 있음"
      ],
      "metadata": {
        "id": "Fd0rg9lCFSJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe 벡터 다운로드 (영어)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "5YiIc8ZVGXAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# GloVe 벡터를 로드하려면 gensim의 KeyedVectors로 변환 필요\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "tmp_file = 'word2vec-glove.6B.100d.txt'\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "# 모델 로드\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "metadata": {
        "id": "MxqtwxSgGdgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'king' 단어 벡터 확인\n",
        "print(\"'king' 단어 벡터:\")\n",
        "print(model['king'])\n",
        "\n",
        "# 유사한 단어 찾기\n",
        "similar_words = model.most_similar('king')\n",
        "print(\"'king'과 유사한 단어들:\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "id": "VJAJWy3xGkaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec과 GloVe 간의 차이점"
      ],
      "metadata": {
        "id": "519js-maFjM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec\n",
        "- 로컬 문맥(window-based) 정보를 사용해 단어의 주변 단어들을 예측하는 방식(예: Skip-gram, CBOW)\n",
        "\n",
        "GloVe\n",
        "- 전체 말뭉치의 동시 발생 통계를 활용해 전역적 관점에서 단어 간 관계를 학습\n"
      ],
      "metadata": {
        "id": "p7-i0j1xFoZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문맥 기반 임베딩 (Contextual Embeddings)"
      ],
      "metadata": {
        "id": "zk0WYTodGp6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "핵심 아이디어\n",
        "- 단어의 의미가 문맥에 따라 달라진다는 점을 반영하여, 각 단어에 대해 문맥을 고려한 동적 벡터를 생성\n",
        "\n",
        "대표 모델\n",
        "- BERT, ELMo 등\n",
        "\n",
        "특징\n",
        "- 문장 내에서 단어의 의미를 보다 정밀하게 파악할 수 있음\n",
        "- 사전 학습된 모델을 활용하여 전이학습 가능"
      ],
      "metadata": {
        "id": "fC-CNZQ3Gtd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "vjcqczBHG7Ud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZuxUNZ0CCXp"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# 사전 학습된 BERT 모델과 토크나이저 로드 (여기서는 'bert-base-uncased' 사용)\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# 평가 모드로 전환 (추론 시 dropout 등을 비활성화)\n",
        "model.eval()\n",
        "\n",
        "# 예제 문장\n",
        "text = \"This is an example sentence for BERT embedding.\"\n",
        "\n",
        "# 텍스트를 토큰화하고 텐서로 변환\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# 모델에 입력하여 출력 얻기 (그라디언트 계산이 필요 없으므로 no_grad() 사용)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded_input)\n",
        "\n",
        "# BERT의 출력은 last_hidden_state와 pooler_output 등으로 구성됨\n",
        "# last_hidden_state: [batch_size, sequence_length, hidden_size]\n",
        "# [CLS] 토큰은 항상 시퀀스의 첫 번째 토큰이므로, 이를 선택하여 문장의 대표 임베딩으로 사용\n",
        "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "print(\"BERT [CLS] Embedding:\\n\", cls_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYcEoUU8Hi5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}